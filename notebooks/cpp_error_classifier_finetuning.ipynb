{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09176c5",
   "metadata": {},
   "source": [
    "# C++ Code Error Classifier - LLM Fine-Tuning with QLoRA\n",
    "\n",
    "This notebook fine-tunes a Large Language Model using **QLoRA (4-bit quantization + LoRA)** to classify C++ code errors based on the difference between \"Original\" (buggy) and \"Fixed\" code.\n",
    "\n",
    "**Environment:** Google Colab Free Tier (T4 GPU, ~15GB VRAM)\n",
    "\n",
    "**Technique:** \n",
    "- 4-bit quantization with `bitsandbytes`\n",
    "- LoRA adapters via `peft`\n",
    "- `unsloth` for optimized training speed\n",
    "\n",
    "**Task:** Given a code diff, predict the error category:\n",
    "- Memory Management\n",
    "- Invalid Access\n",
    "- Uninitialized\n",
    "- Concurrency\n",
    "- Logic Error\n",
    "- Resource Leak\n",
    "- Security/Portability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d4d80",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "Install all required dependencies optimized for Google Colab. Using Unsloth's recommended installation for maximum compatibility and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750502d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth (optimized for Colab) - this handles all dependencies\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Install additional required packages\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "# Install visualization and ML utilities\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8788c",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739dfaed",
   "metadata": {},
   "source": [
    "## 3. Configuration & Constants\n",
    "\n",
    "Define the error categories and label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these paths and parameters as needed\n",
    "# =============================================================================\n",
    "\n",
    "# Path to folder containing .json files (update this for your data location)\n",
    "DATA_FOLDER = \"/content/drive/MyDrive/fixmycodedb_export\"  # Change to your path\n",
    "\n",
    "# Error category mappings (snake_case -> Human Readable)\n",
    "LABEL_MAPPING = {\n",
    "    \"memory_management\": \"Memory Management\",\n",
    "    \"invalid_access\": \"Invalid Access\",\n",
    "    \"uninitialized\": \"Uninitialized\",\n",
    "    \"concurrency\": \"Concurrency\",\n",
    "    \"logic_error\": \"Logic Error\",\n",
    "    \"resource_leak\": \"Resource Leak\",\n",
    "    \"security_portability\": \"Security/Portability\",\n",
    "    \"code_quality_performance\": \"Code Quality/Performance\"\n",
    "}\n",
    "\n",
    "# All valid labels (for classification)\n",
    "VALID_LABELS = list(LABEL_MAPPING.values())\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"  # or \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# Training configuration (optimized for Colab Free Tier)\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "MAX_STEPS = 60  # Adjust based on dataset size\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Target labels: {VALID_LABELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925ac56",
   "metadata": {},
   "source": [
    "## 4. Data Loading Function\n",
    "\n",
    "Load all JSON files from the specified directory and extract the relevant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(folder_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load all .json files from a directory and extract relevant fields.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to the folder containing .json files\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with code_original, code_fixed, and labels\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files in {folder_path}\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                record = json.load(f)\n",
    "            \n",
    "            # Extract required fields\n",
    "            code_original = record.get(\"code_original\", \"\")\n",
    "            code_fixed = record.get(\"code_fixed\", \"\")\n",
    "            labels_groups = record.get(\"labels\", {}).get(\"groups\", {})\n",
    "            \n",
    "            # Skip if essential fields are missing\n",
    "            if not code_original or not labels_groups:\n",
    "                continue\n",
    "            \n",
    "            data.append({\n",
    "                \"code_original\": code_original,\n",
    "                \"code_fixed\": code_fixed if code_fixed else code_original,  # Fallback\n",
    "                \"labels_groups\": labels_groups,\n",
    "                \"file_name\": os.path.basename(file_path)\n",
    "            })\n",
    "            \n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully loaded {len(data)} records\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# For testing without actual files, create sample data\n",
    "def create_sample_data() -> List[Dict]:\n",
    "    \"\"\"Create sample data for testing the pipeline.\"\"\"\n",
    "    samples = [\n",
    "        {\n",
    "            \"code_original\": \"int* ptr = malloc(sizeof(int));\\n*ptr = 5;\\nreturn 0;\",\n",
    "            \"code_fixed\": \"int* ptr = malloc(sizeof(int));\\n*ptr = 5;\\nfree(ptr);\\nreturn 0;\",\n",
    "            \"labels_groups\": {\"memory_management\": False, \"resource_leak\": True, \"logic_error\": False},\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"int arr[5];\\nfor(int i=0; i<=5; i++) arr[i] = i;\",\n",
    "            \"code_fixed\": \"int arr[5];\\nfor(int i=0; i<5; i++) arr[i] = i;\",\n",
    "            \"labels_groups\": {\"invalid_access\": True, \"logic_error\": False, \"memory_management\": False},\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"int x;\\nprintf(\\\"%d\\\", x);\",\n",
    "            \"code_fixed\": \"int x = 0;\\nprintf(\\\"%d\\\", x);\",\n",
    "            \"labels_groups\": {\"uninitialized\": True, \"logic_error\": False},\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"if (a = 5) { doSomething(); }\",\n",
    "            \"code_fixed\": \"if (a == 5) { doSomething(); }\",\n",
    "            \"labels_groups\": {\"logic_error\": True, \"memory_management\": False},\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"pthread_mutex_lock(&m);\\nif(cond) return;\\npthread_mutex_unlock(&m);\",\n",
    "            \"code_fixed\": \"pthread_mutex_lock(&m);\\nif(cond) { pthread_mutex_unlock(&m); return; }\\npthread_mutex_unlock(&m);\",\n",
    "            \"labels_groups\": {\"concurrency\": True, \"resource_leak\": False},\n",
    "        },\n",
    "    ]\n",
    "    # Duplicate samples to have more data for testing\n",
    "    return samples * 20  # 100 samples for testing\n",
    "\n",
    "\n",
    "# Load data - uncomment the appropriate line\n",
    "raw_data = load_json_files('./exported')  # Use this for real data\n",
    "# raw_data = create_sample_data()  # Use this for testing\n",
    "\n",
    "print(f\"\\nTotal samples: {len(raw_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af8979",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing & Label Extraction\n",
    "\n",
    "Extract the active label from each record and convert to human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd19b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_label(labels_groups: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Extract the active label (the key where value is True).\n",
    "    Returns the human-readable label name.\n",
    "    \n",
    "    Args:\n",
    "        labels_groups: Dictionary of label_name -> boolean\n",
    "        \n",
    "    Returns:\n",
    "        Human-readable label string or \"Unknown\" if none found\n",
    "    \"\"\"\n",
    "    for key, value in labels_groups.items():\n",
    "        if value is True and key in LABEL_MAPPING:\n",
    "            return LABEL_MAPPING[key]\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def preprocess_data(raw_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Preprocess raw data: extract labels, filter invalid entries.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: List of raw records from JSON files\n",
    "        \n",
    "    Returns:\n",
    "        List of processed records with extracted labels\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    label_counts = {}\n",
    "    \n",
    "    for record in raw_data:\n",
    "        label = extract_active_label(record[\"labels_groups\"])\n",
    "        \n",
    "        # Skip unknown labels\n",
    "        if label == \"Unknown\":\n",
    "            continue\n",
    "        \n",
    "        processed.append({\n",
    "            \"code_original\": record[\"code_original\"],\n",
    "            \"code_fixed\": record[\"code_fixed\"],\n",
    "            \"label\": label\n",
    "        })\n",
    "        \n",
    "        # Track label distribution\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"  {label}: {count}\")\n",
    "    \n",
    "    return processed\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "processed_data = preprocess_data(raw_data)\n",
    "print(f\"\\nProcessed samples: {len(processed_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe86880",
   "metadata": {},
   "source": [
    "## 6. Prompt Formatting (Alpaca Style)\n",
    "\n",
    "Format the data into instruction-response pairs suitable for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca-style prompt template with few-shot context\n",
    "ALPACA_PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are a C++ code error classifier. Analyze the difference between the original (buggy) code and the fixed code, then classify the error into exactly ONE of these categories:\n",
    "- Memory Management: Issues with malloc/free, new/delete, memory allocation\n",
    "- Invalid Access: Array out of bounds, null pointer dereference, invalid memory access\n",
    "- Uninitialized: Using uninitialized variables\n",
    "- Concurrency: Race conditions, deadlocks, thread safety issues\n",
    "- Logic Error: Incorrect conditions, off-by-one errors, wrong operators\n",
    "- Resource Leak: Unclosed files, sockets, unreleased resources\n",
    "- Security/Portability: Security vulnerabilities, platform-specific issues\n",
    "- Code Quality/Performance: Style issues, inefficient code\n",
    "\n",
    "Respond with ONLY the category name, nothing else.\n",
    "\n",
    "### Input:\n",
    "Original Code:\n",
    "```cpp\n",
    "{code_original}\n",
    "```\n",
    "\n",
    "Fixed Code:\n",
    "```cpp\n",
    "{code_fixed}\n",
    "```\n",
    "\n",
    "### Response:\n",
    "{label}\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt(sample: Dict, include_response: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Format a sample into Alpaca-style prompt.\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary with code_original, code_fixed, and label\n",
    "        include_response: Whether to include the label (for training) or not (for inference)\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = ALPACA_PROMPT_TEMPLATE.format(\n",
    "        code_original=sample[\"code_original\"].strip(),\n",
    "        code_fixed=sample[\"code_fixed\"].strip(),\n",
    "        label=sample[\"label\"] if include_response else \"\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def format_for_inference(sample: Dict) -> str:\n",
    "    \"\"\"Format prompt for inference (without the response).\"\"\"\n",
    "    return format_prompt(sample, include_response=False).rstrip()\n",
    "\n",
    "\n",
    "# Test the formatting\n",
    "print(\"Example formatted prompt:\")\n",
    "print(\"=\" * 80)\n",
    "example_prompt = format_prompt(processed_data[0])\n",
    "print(example_prompt[:1500] + \"...\" if len(example_prompt) > 1500 else example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e96fd",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split\n",
    "\n",
    "Split the data with stratification to ensure balanced label representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels for stratification\n",
    "labels = [sample[\"label\"] for sample in processed_data]\n",
    "\n",
    "# Split 80% train, 20% test with stratification\n",
    "train_data, test_data = train_test_split(\n",
    "    processed_data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")\n",
    "\n",
    "# Verify stratification\n",
    "train_labels = [s[\"label\"] for s in train_data]\n",
    "test_labels = [s[\"label\"] for s in test_data]\n",
    "\n",
    "print(\"\\nTraining set distribution:\")\n",
    "for label in set(train_labels):\n",
    "    print(f\"  {label}: {train_labels.count(label)}\")\n",
    "\n",
    "print(\"\\nTest set distribution:\")\n",
    "for label in set(test_labels):\n",
    "    print(f\"  {label}: {test_labels.count(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fec4ca",
   "metadata": {},
   "source": [
    "## 8. Load Base Model with 4-bit Quantization\n",
    "\n",
    "Load the pre-quantized model using Unsloth for optimized memory usage and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c88c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect (float16 for T4)\n",
    "    load_in_4bit=True,  # QLoRA 4-bit quantization\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567761e",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA Adapters\n",
    "\n",
    "Apply LoRA configuration to enable efficient fine-tuning with minimal trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63570d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,  # Rank - higher = more parameters, more memory\n",
    "    lora_alpha=LORA_ALPHA,  # Scaling factor\n",
    "    lora_dropout=LORA_DROPOUT,  # No dropout for stability\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention modules\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",  # MLP modules\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth optimized checkpointing\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7c49d",
   "metadata": {},
   "source": [
    "## 10. Prepare Dataset for Training\n",
    "\n",
    "Convert the formatted training data into a Hugging Face Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd30973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format training data\n",
    "formatted_train_data = [\n",
    "    {\"text\": format_prompt(sample)} for sample in train_data\n",
    "]\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "train_dataset = Dataset.from_list(formatted_train_data)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(\"\\nSample formatted text (first 500 chars):\")\n",
    "print(train_dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332bba2",
   "metadata": {},
   "source": [
    "## 10.5 Baseline Evaluation (Before Fine-Tuning)\n",
    "\n",
    "Run inference on the test set using the base model BEFORE training to establish a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to inference mode for baseline evaluation\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_prediction_baseline(sample: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prediction for a single sample (baseline - before fine-tuning).\n",
    "    \"\"\"\n",
    "    # Format the prompt (without response)\n",
    "    prompt = format_for_inference(sample)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH - 50\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            use_cache=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return generated.strip()\n",
    "\n",
    "\n",
    "# Run baseline inference on test set\n",
    "print(\"Running BASELINE inference (before fine-tuning)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "baseline_predictions = []\n",
    "baseline_true_labels = []\n",
    "\n",
    "for i, sample in enumerate(test_data):\n",
    "    pred = generate_prediction_baseline(sample)\n",
    "    baseline_predictions.append(pred)\n",
    "    baseline_true_labels.append(sample[\"label\"])\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(test_data)} samples...\")\n",
    "\n",
    "# Parse baseline predictions\n",
    "baseline_parsed = [parse_prediction(p) for p in baseline_predictions]\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "baseline_correct = sum(1 for pred, true in zip(baseline_parsed, baseline_true_labels) if pred == true)\n",
    "baseline_total = len(baseline_true_labels)\n",
    "baseline_accuracy = (baseline_correct / baseline_total) * 100\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"BASELINE ACCURACY (Before Fine-Tuning): {baseline_accuracy:.2f}%\")\n",
    "print(f\"Correct: {baseline_correct} / {baseline_total}\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Show some baseline predictions\n",
    "print(\"\\nSample baseline predictions:\")\n",
    "for i in range(min(5, len(test_data))):\n",
    "    print(f\"True: {baseline_true_labels[i]:<25} | Pred: {baseline_parsed[i]}\")\n",
    "\n",
    "# Switch back to training mode for fine-tuning\n",
    "model.train()\n",
    "print(\"\\nModel set back to training mode. Proceeding to fine-tuning...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cac0b",
   "metadata": {},
   "source": [
    "## 11. Configure SFTTrainer\n",
    "\n",
    "Set up the Supervised Fine-tuning Trainer with hyperparameters optimized for Colab Free Tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24936c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=5,\n",
    "    max_steps=MAX_STEPS,  # Or use num_train_epochs=1 for full epoch\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # Use fp16 if bf16 not supported\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for Colab\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Disable packing for classification tasks\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer configured successfully!\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Max training steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357bb01",
   "metadata": {},
   "source": [
    "## 12. Run Training\n",
    "\n",
    "Execute the fine-tuning process and monitor memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record starting memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Report memory usage\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    end_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Peak GPU memory used: {peak_memory:.2f} GB\")\n",
    "    print(f\"Current GPU memory: {end_memory:.2f} GB\")\n",
    "    print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3f7d1",
   "metadata": {},
   "source": [
    "## 13. Inference on Test Set\n",
    "\n",
    "Run predictions on the test set using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_prediction(sample: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prediction for a single sample.\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary with code_original and code_fixed\n",
    "        \n",
    "    Returns:\n",
    "        Model's predicted label\n",
    "    \"\"\"\n",
    "    # Format the prompt (without response)\n",
    "    prompt = format_for_inference(sample)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH - 50  # Leave room for generation\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            use_cache=True,\n",
    "            do_sample=False,  # Greedy decoding for classification\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return generated.strip()\n",
    "\n",
    "\n",
    "# Run inference on test set\n",
    "print(\"Running inference on test set...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for i, sample in enumerate(test_data):\n",
    "    pred = generate_prediction(sample)\n",
    "    predictions.append(pred)\n",
    "    true_labels.append(sample[\"label\"])\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(test_data)} samples...\")\n",
    "\n",
    "print(f\"\\nInference complete! Processed {len(test_data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21764b3d",
   "metadata": {},
   "source": [
    "## 14. Parse Predictions & Calculate Accuracy\n",
    "\n",
    "Extract and normalize predictions, then calculate classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f58f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prediction(raw_prediction: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse and normalize the model's prediction to match valid labels.\n",
    "    \n",
    "    Args:\n",
    "        raw_prediction: Raw model output string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized label or \"Unknown\" if not recognized\n",
    "    \"\"\"\n",
    "    # Clean up the prediction\n",
    "    pred = raw_prediction.strip().split(\"\\n\")[0]  # Take first line only\n",
    "    pred = pred.strip(\".,!?;:\")  # Remove trailing punctuation\n",
    "    \n",
    "    # Try exact match first\n",
    "    for label in VALID_LABELS:\n",
    "        if label.lower() == pred.lower():\n",
    "            return label\n",
    "    \n",
    "    # Try partial match (label contained in prediction)\n",
    "    for label in VALID_LABELS:\n",
    "        if label.lower() in pred.lower():\n",
    "            return label\n",
    "    \n",
    "    # Try matching key words\n",
    "    keyword_map = {\n",
    "        \"memory\": \"Memory Management\",\n",
    "        \"invalid\": \"Invalid Access\",\n",
    "        \"access\": \"Invalid Access\",\n",
    "        \"uninitialized\": \"Uninitialized\",\n",
    "        \"uninit\": \"Uninitialized\",\n",
    "        \"concurrent\": \"Concurrency\",\n",
    "        \"race\": \"Concurrency\",\n",
    "        \"thread\": \"Concurrency\",\n",
    "        \"logic\": \"Logic Error\",\n",
    "        \"resource\": \"Resource Leak\",\n",
    "        \"leak\": \"Resource Leak\",\n",
    "        \"security\": \"Security/Portability\",\n",
    "        \"portability\": \"Security/Portability\",\n",
    "        \"quality\": \"Code Quality/Performance\",\n",
    "        \"performance\": \"Code Quality/Performance\",\n",
    "    }\n",
    "    \n",
    "    pred_lower = pred.lower()\n",
    "    for keyword, label in keyword_map.items():\n",
    "        if keyword in pred_lower:\n",
    "            return label\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "# Parse all predictions\n",
    "parsed_predictions = [parse_prediction(p) for p in predictions]\n",
    "\n",
    "# Show some examples\n",
    "print(\"Sample predictions:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(test_data))):\n",
    "    print(f\"True: {true_labels[i]:<25} | Raw: {predictions[i][:30]:<30} | Parsed: {parsed_predictions[i]}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum(1 for pred, true in zip(parsed_predictions, true_labels) if pred == true)\n",
    "total = len(true_labels)\n",
    "accuracy = (correct / total) * 100\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"CLASSIFICATION ACCURACY: {accuracy:.2f}%\")\n",
    "print(f\"Correct: {correct} / {total}\")\n",
    "print(f\"{'=' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e985e",
   "metadata": {},
   "source": [
    "## 15. Generate Confusion Matrix Visualization\n",
    "\n",
    "Create visualizations to understand model performance across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique labels from both predictions and true labels\n",
    "all_labels = sorted(set(true_labels + parsed_predictions))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, parsed_predictions, labels=all_labels)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Confusion Matrix Heatmap\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=all_labels,\n",
    "    yticklabels=all_labels,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_title(f'Confusion Matrix\\n(Accuracy: {accuracy:.2f}%)', fontsize=14)\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "plt.setp(ax1.get_yticklabels(), rotation=0, fontsize=9)\n",
    "\n",
    "# Plot 2: Bar chart - Predicted vs Actual counts\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Count occurrences\n",
    "true_counts = {label: true_labels.count(label) for label in all_labels}\n",
    "pred_counts = {label: parsed_predictions.count(label) for label in all_labels}\n",
    "\n",
    "x = range(len(all_labels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar([i - width/2 for i in x], [true_counts.get(l, 0) for l in all_labels], \n",
    "                width, label='Actual', color='steelblue', alpha=0.8)\n",
    "bars2 = ax2.bar([i + width/2 for i in x], [pred_counts.get(l, 0) for l in all_labels], \n",
    "                width, label='Predicted', color='coral', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Error Category', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Actual vs Predicted Distribution', fontsize=14)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(all_labels, rotation=45, ha='right', fontsize=9)\n",
    "ax2.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{int(height)}',\n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                 xytext=(0, 3),\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{int(height)}',\n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                 xytext=(0, 3),\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to 'classification_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c43ccb",
   "metadata": {},
   "source": [
    "## 16. Display Results Summary\n",
    "\n",
    "Final summary with classification report and memory statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3790a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(true_labels, parsed_predictions, labels=all_labels, zero_division=0))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"LoRA rank: {LORA_R}, alpha: {LORA_ALPHA}\")\n",
    "print(f\"Training steps: {MAX_STEPS}\")\n",
    "print(f\"Batch size (effective): {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Correct predictions: {correct}/{total}\")\n",
    "\n",
    "# Memory usage summary\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"-\" * 60)\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    print(f\"  Peak memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Current memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f34ca",
   "metadata": {},
   "source": [
    "## 16.5 Baseline vs Fine-Tuned Comparison\n",
    "\n",
    "Compare the performance of the base model (before training) with the fine-tuned model to evaluate the effectiveness of the training data and LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b25983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BASELINE VS FINE-TUNED COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE VS FINE-TUNED MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = accuracy - baseline_accuracy\n",
    "relative_improvement = ((accuracy - baseline_accuracy) / max(baseline_accuracy, 0.01)) * 100\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Baseline':<15} {'Fine-Tuned':<15} {'Change':<15}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Accuracy (%)':<30} {baseline_accuracy:<15.2f} {accuracy:<15.2f} {'+' if improvement >= 0 else ''}{improvement:.2f}\")\n",
    "print(f\"{'Correct Predictions':<30} {baseline_correct:<15} {correct:<15} {'+' if (correct - baseline_correct) >= 0 else ''}{correct - baseline_correct}\")\n",
    "print(f\"{'Total Samples':<30} {baseline_total:<15} {total:<15} {'N/A':<15}\")\n",
    "\n",
    "# Determine if fine-tuning was effective\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if improvement > 5:\n",
    "    print(\"✅ FINE-TUNING EFFECTIVE: Significant accuracy improvement!\")\n",
    "    print(f\"   The model improved by {improvement:.2f} percentage points ({relative_improvement:.1f}% relative).\")\n",
    "    print(\"   The training dataset appears to be effective for this task.\")\n",
    "elif improvement > 0:\n",
    "    print(\"⚠️ MARGINAL IMPROVEMENT: Fine-tuning showed slight gains.\")\n",
    "    print(f\"   The model improved by {improvement:.2f} percentage points.\")\n",
    "    print(\"   Consider: More training data, more training steps, or hyperparameter tuning.\")\n",
    "elif improvement == 0:\n",
    "    print(\"⚪ NO CHANGE: Fine-tuning did not affect accuracy.\")\n",
    "    print(\"   Consider: Different learning rate, more diverse training data, or longer training.\")\n",
    "else:\n",
    "    print(\"❌ PERFORMANCE DEGRADATION: Fine-tuning reduced accuracy!\")\n",
    "    print(f\"   The model degraded by {abs(improvement):.2f} percentage points.\")\n",
    "    print(\"   Possible causes: Overfitting, poor data quality, or suboptimal hyperparameters.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Accuracy comparison bar chart\n",
    "ax1 = axes[0]\n",
    "models = ['Baseline\\n(No Fine-Tuning)', 'Fine-Tuned\\n(LoRA)']\n",
    "accuracies = [baseline_accuracy, accuracy]\n",
    "colors = ['lightcoral' if baseline_accuracy < accuracy else 'steelblue', \n",
    "          'forestgreen' if accuracy > baseline_accuracy else 'steelblue']\n",
    "bars = ax1.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Model Accuracy Comparison', fontsize=14)\n",
    "ax1.set_ylim(0, 100)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.annotate(f'{acc:.1f}%', \n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                 xytext=(0, 5), textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add improvement arrow\n",
    "if improvement != 0:\n",
    "    mid_x = 0.5\n",
    "    ax1.annotate('', xy=(1, accuracy), xytext=(0, baseline_accuracy),\n",
    "                 arrowprops=dict(arrowstyle='->', color='green' if improvement > 0 else 'red', lw=2))\n",
    "    ax1.text(0.5, (baseline_accuracy + accuracy) / 2, \n",
    "             f'{\"+%.1f%%\" % improvement if improvement > 0 else \"%.1f%%\" % improvement}',\n",
    "             ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "             color='green' if improvement > 0 else 'red',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "# Plot 2: Per-category comparison\n",
    "ax2 = axes[1]\n",
    "categories = sorted(set(baseline_true_labels))\n",
    "baseline_per_cat = {cat: sum(1 for p, t in zip(baseline_parsed, baseline_true_labels) if t == cat and p == t) / \n",
    "                         max(baseline_true_labels.count(cat), 1) * 100 for cat in categories}\n",
    "finetuned_per_cat = {cat: sum(1 for p, t in zip(parsed_predictions, true_labels) if t == cat and p == t) / \n",
    "                          max(true_labels.count(cat), 1) * 100 for cat in categories}\n",
    "\n",
    "x = range(len(categories))\n",
    "width = 0.35\n",
    "bars1 = ax2.bar([i - width/2 for i in x], [baseline_per_cat.get(c, 0) for c in categories], \n",
    "                width, label='Baseline', color='lightcoral', alpha=0.8)\n",
    "bars2 = ax2.bar([i + width/2 for i in x], [finetuned_per_cat.get(c, 0) for c in categories], \n",
    "                width, label='Fine-Tuned', color='forestgreen', alpha=0.8)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Per-Category Accuracy Comparison', fontsize=14)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(categories, rotation=45, ha='right', fontsize=9)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 110)\n",
    "\n",
    "# Plot 3: Confusion matrix comparison (side by side text summary)\n",
    "ax3 = axes[2]\n",
    "ax3.axis('off')\n",
    "\n",
    "# Create text summary\n",
    "summary_text = f\"\"\"\n",
    "TRAINING EFFECTIVENESS SUMMARY\n",
    "{'=' * 40}\n",
    "\n",
    "Base Model: {MODEL_NAME.split('/')[-1]}\n",
    "Training Steps: {MAX_STEPS}\n",
    "LoRA Rank: {LORA_R}\n",
    "Learning Rate: {LEARNING_RATE}\n",
    "\n",
    "{'=' * 40}\n",
    "RESULTS\n",
    "{'=' * 40}\n",
    "\n",
    "Baseline Accuracy:  {baseline_accuracy:>6.2f}%\n",
    "Fine-Tuned Accuracy:{accuracy:>6.2f}%\n",
    "                    ────────\n",
    "Improvement:        {'+' if improvement >= 0 else ''}{improvement:>6.2f}%\n",
    "\n",
    "{'=' * 40}\n",
    "VERDICT: {'✅ EFFECTIVE' if improvement > 5 else '⚠️ MARGINAL' if improvement > 0 else '❌ INEFFECTIVE'}\n",
    "{'=' * 40}\n",
    "\n",
    "Recommendations:\n",
    "{'• Training data is effective!' if improvement > 5 else '• Consider more training data' if improvement > 0 else '• Review data quality'}\n",
    "{'• Model learned task-specific patterns' if improvement > 5 else '• Try more training steps' if improvement >= 0 else '• Check for overfitting'}\n",
    "{'• Ready for deployment' if improvement > 10 else '• May need hyperparameter tuning' if improvement > 0 else '• Reconsider approach'}\n",
    "\"\"\"\n",
    "\n",
    "ax3.text(0.1, 0.95, summary_text, transform=ax3.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_vs_finetuned_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison visualization saved to 'baseline_vs_finetuned_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c82e9",
   "metadata": {},
   "source": [
    "## 17. Save the Fine-tuned Model (Optional)\n",
    "\n",
    "Save the LoRA adapters for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e35825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters only (small file, ~50-100 MB)\n",
    "model.save_pretrained(\"cpp_error_classifier_lora\")\n",
    "tokenizer.save_pretrained(\"cpp_error_classifier_lora\")\n",
    "print(\"LoRA adapters saved to 'cpp_error_classifier_lora/'\")\n",
    "\n",
    "# Optional: Save to Google Drive (uncomment if needed)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp -r cpp_error_classifier_lora /content/drive/MyDrive/\n",
    "\n",
    "# Optional: Merge and save full model (requires more storage)\n",
    "# model.save_pretrained_merged(\"cpp_error_classifier_merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "print(\"\\nTo load the saved model later:\")\n",
    "print(\"```python\")\n",
    "print(\"from unsloth import FastLanguageModel\")\n",
    "print(\"model, tokenizer = FastLanguageModel.from_pretrained('cpp_error_classifier_lora')\")\n",
    "print(\"```\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
