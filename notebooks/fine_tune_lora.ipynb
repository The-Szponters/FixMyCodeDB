{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch transformers peft bitsandbytes accelerate datasets trl matplotlib pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157d791",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28baddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Configuration\n",
    "# ============================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Lightweight, fits in 12GB VRAM\n",
    "# Alternative models:\n",
    "# MODEL_NAME = \"microsoft/phi-2\"  # 2.7B params, good for code\n",
    "# MODEL_NAME = \"codellama/CodeLlama-7b-Instruct-hf\"  # Needs more VRAM\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = \"../export.json\"  # Path to exported data from FixMyCodeDB\n",
    "OUTPUT_DIR = \"./lora_finetuned_model\"\n",
    "\n",
    "# Training configuration\n",
    "TRAIN_TEST_SPLIT = 0.2  # 80% train, 20% test\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "BATCH_SIZE = 2  # Adjust based on VRAM\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16  # Rank of the low-rank matrices\n",
    "LORA_ALPHA = 32  # Scaling factor\n",
    "LORA_DROPOUT = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc4c55",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b490453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from JSON or CSV file exported from FixMyCodeDB.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the data file\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with the loaded data\n",
    "    \"\"\"\n",
    "    path = Path(data_path)\n",
    "\n",
    "    if path.suffix == \".json\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "    elif path.suffix == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {path.suffix}\")\n",
    "\n",
    "    print(f\"Loaded {len(df)} entries from {data_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Try to load data, or create sample data for demonstration\n",
    "try:\n",
    "    df = load_data(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Data file not found at {DATA_PATH}. Creating sample data for demonstration.\")\n",
    "    # Sample data for demonstration\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"code_original\": \"int* ptr = malloc(sizeof(int));\\nfree(ptr);\\nfree(ptr);  // Double free!\",\n",
    "            \"code_fixed\": \"int* ptr = malloc(sizeof(int));\\nfree(ptr);\\nptr = NULL;\",\n",
    "            \"labels\": {\"cppcheck\": [\"doubleFree\"], \"groups\": {\"memory_management\": True}}\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"int arr[10];\\nint x = arr[15];  // Out of bounds!\",\n",
    "            \"code_fixed\": \"int arr[10];\\nint x = arr[5];  // Valid index\",\n",
    "            \"labels\": {\"cppcheck\": [\"arrayIndexOutOfBounds\"], \"groups\": {\"invalid_access\": True}}\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"int x;\\nprintf(\\\"%d\\\", x);  // Uninitialized!\",\n",
    "            \"code_fixed\": \"int x = 0;\\nprintf(\\\"%d\\\", x);\",\n",
    "            \"labels\": {\"cppcheck\": [\"uninitvar\"], \"groups\": {\"uninitialized\": True}}\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"int* ptr = NULL;\\n*ptr = 5;  // Null pointer dereference!\",\n",
    "            \"code_fixed\": \"int val = 5;\\nint* ptr = &val;\\n*ptr = 5;\",\n",
    "            \"labels\": {\"cppcheck\": [\"nullPointer\"], \"groups\": {\"invalid_access\": True}}\n",
    "        },\n",
    "        {\n",
    "            \"code_original\": \"FILE* f = fopen(\\\"test.txt\\\", \\\"r\\\");\\n// No fclose - resource leak!\",\n",
    "            \"code_fixed\": \"FILE* f = fopen(\\\"test.txt\\\", \\\"r\\\");\\nif (f) { /* use file */ fclose(f); }\",\n",
    "            \"labels\": {\"cppcheck\": [\"resourceLeak\"], \"groups\": {\"resource_leak\": True}}\n",
    "        },\n",
    "    ]\n",
    "    df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cd616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(labels_data) -> str:\n",
    "    \"\"\"\n",
    "    Extract labels from the nested labels structure.\n",
    "\n",
    "    Args:\n",
    "        labels_data: Labels dict or string\n",
    "\n",
    "    Returns:\n",
    "        Comma-separated string of labels\n",
    "    \"\"\"\n",
    "    if isinstance(labels_data, str):\n",
    "        return labels_data\n",
    "\n",
    "    if isinstance(labels_data, dict):\n",
    "        cppcheck = labels_data.get(\"cppcheck\", [])\n",
    "        if isinstance(cppcheck, list):\n",
    "            return \", \".join(cppcheck)\n",
    "        return str(cppcheck)\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def extract_categories(labels_data) -> str:\n",
    "    \"\"\"\n",
    "    Extract active category groups from labels.\n",
    "\n",
    "    Args:\n",
    "        labels_data: Labels dict\n",
    "\n",
    "    Returns:\n",
    "        Comma-separated string of active categories\n",
    "    \"\"\"\n",
    "    if not isinstance(labels_data, dict):\n",
    "        return \"unknown\"\n",
    "\n",
    "    groups = labels_data.get(\"groups\", {})\n",
    "    if not isinstance(groups, dict):\n",
    "        return \"unknown\"\n",
    "\n",
    "    active = [k.replace(\"_\", \" \") for k, v in groups.items() if v]\n",
    "    return \", \".join(active) if active else \"none\"\n",
    "\n",
    "\n",
    "# Process labels\n",
    "df[\"bug_labels\"] = df[\"labels\"].apply(extract_labels)\n",
    "df[\"bug_categories\"] = df[\"labels\"].apply(extract_categories)\n",
    "\n",
    "print(\"Sample processed data:\")\n",
    "print(df[[\"bug_labels\", \"bug_categories\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240358a",
   "metadata": {},
   "source": [
    "## 3. Create Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8432b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for bug detection instruction tuning\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert C++ code analyzer. Your task is to identify bugs and issues in C++ code.\n",
    "When analyzing code, identify:\n",
    "1. The specific bug type (e.g., memory leak, null pointer, buffer overflow)\n",
    "2. The category of the bug (e.g., memory management, invalid access, uninitialized variable)\n",
    "3. A brief explanation of why this is a bug\n",
    "4. A suggested fix\n",
    "\"\"\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Analyze the following C++ code for bugs:\n",
    "\n",
    "```cpp\n",
    "{code}\n",
    "```\n",
    "\n",
    "Identify any bugs present in this code.\"\"\"\n",
    "\n",
    "ASSISTANT_TEMPLATE = \"\"\"## Bug Analysis\n",
    "\n",
    "**Bug Type(s):** {bug_labels}\n",
    "\n",
    "**Category:** {bug_categories}\n",
    "\n",
    "**Explanation:** The code contains the following issue(s): {bug_labels}. This falls under the {bug_categories} category.\n",
    "\n",
    "**Suggested Fix:**\n",
    "```cpp\n",
    "{fixed_code}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_conversation(row) -> str:\n",
    "    \"\"\"\n",
    "    Create a conversation-style prompt for fine-tuning.\n",
    "    Uses TinyLlama chat format.\n",
    "\n",
    "    Args:\n",
    "        row: DataFrame row with code and labels\n",
    "\n",
    "    Returns:\n",
    "        Formatted conversation string\n",
    "    \"\"\"\n",
    "    code = row.get(\"code_original\", \"\")\n",
    "    fixed = row.get(\"code_fixed\", \"\")\n",
    "    labels = row.get(\"bug_labels\", \"unknown\")\n",
    "    categories = row.get(\"bug_categories\", \"unknown\")\n",
    "\n",
    "    # TinyLlama chat format\n",
    "    conversation = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}</s>\n",
    "<|user|>\n",
    "{USER_TEMPLATE.format(code=code)}</s>\n",
    "<|assistant|>\n",
    "{ASSISTANT_TEMPLATE.format(\n",
    "    bug_labels=labels,\n",
    "    bug_categories=categories,\n",
    "    fixed_code=fixed if fixed else 'See explanation for fix suggestions.'\n",
    ")}</s>\"\"\"\n",
    "\n",
    "    return conversation\n",
    "\n",
    "\n",
    "# Create training prompts\n",
    "df[\"text\"] = df.apply(create_conversation, axis=1)\n",
    "\n",
    "print(\"Sample conversation prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(df[\"text\"].iloc[0])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1fce1",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TRAIN_TEST_SPLIT,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Testing samples: {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"text\"]])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset: {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2791e",
   "metadata": {},
   "source": [
    "## 5. Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2252cc",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80740e",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27230003",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1165b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with SFTTrainer (Supervised Fine-Tuning)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daf1b2",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training history\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# Separate training and eval metrics\n",
    "train_losses = [(h[\"step\"], h[\"loss\"]) for h in history if \"loss\" in h and \"eval_loss\" not in h]\n",
    "eval_losses = [(h[\"step\"], h[\"eval_loss\"]) for h in history if \"eval_loss\" in h]\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "if train_losses:\n",
    "    steps, losses = zip(*train_losses)\n",
    "    axes[0].plot(steps, losses, \"b-\", label=\"Training Loss\", linewidth=2)\n",
    "    axes[0].set_xlabel(\"Training Steps\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training Loss Over Time\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Evaluation Loss (proxy for accuracy improvement)\n",
    "if eval_losses:\n",
    "    steps, losses = zip(*eval_losses)\n",
    "    axes[1].plot(steps, losses, \"r-\", label=\"Evaluation Loss\", linewidth=2, marker=\"o\")\n",
    "    axes[1].set_xlabel(\"Training Steps\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_title(\"Evaluation Loss Over Time\\n(Lower = Better Performance)\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_progress.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining visualization saved to {OUTPUT_DIR}/training_progress.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization: Loss reduction percentage\n",
    "if train_losses and len(train_losses) > 1:\n",
    "    initial_loss = train_losses[0][1]\n",
    "    final_loss = train_losses[-1][1]\n",
    "    reduction = ((initial_loss - final_loss) / initial_loss) * 100\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(\"Training Summary\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"Initial training loss: {initial_loss:.4f}\")\n",
    "    print(f\"Final training loss:   {final_loss:.4f}\")\n",
    "    print(f\"Loss reduction:        {reduction:.1f}%\")\n",
    "\n",
    "    if eval_losses:\n",
    "        print(f\"\\nBest evaluation loss:  {min(l for _, l in eval_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f27a8d",
   "metadata": {},
   "source": [
    "## 10. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "adapter_path = f\"{OUTPUT_DIR}/lora_adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"LoRA adapter saved to: {adapter_path}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "for f in os.listdir(adapter_path):\n",
    "    size = os.path.getsize(os.path.join(adapter_path, f)) / 1e6\n",
    "    print(f\"  - {f}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d70d664",
   "metadata": {},
   "source": [
    "## 11. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461249c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code(code: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Analyze C++ code for bugs using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        code: C++ code to analyze\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Model tokenizer\n",
    "\n",
    "    Returns:\n",
    "        Model's analysis of the code\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}</s>\n",
    "<|user|>\n",
    "{USER_TEMPLATE.format(code=code)}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract assistant response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1]\n",
    "    if \"</s>\" in response:\n",
    "        response = response.split(\"</s>\")[0]\n",
    "\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with sample code\n",
    "test_code = \"\"\"\n",
    "void processData() {\n",
    "    int* data = new int[100];\n",
    "    // Process data...\n",
    "    if (error_condition) {\n",
    "        return;  // Memory leak - data not deleted!\n",
    "    }\n",
    "    delete[] data;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Test Code:\")\n",
    "print(\"=\" * 60)\n",
    "print(test_code)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nModel Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "analysis = analyze_code(test_code, model, tokenizer)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with another example\n",
    "test_code_2 = \"\"\"\n",
    "char* getName() {\n",
    "    char name[50];\n",
    "    strcpy(name, \"John Doe\");\n",
    "    return name;  // Returning address of local variable!\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Test Code 2:\")\n",
    "print(\"=\" * 60)\n",
    "print(test_code_2)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nModel Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "analysis_2 = analyze_code(test_code_2, model, tokenizer)\n",
    "print(analysis_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2fd133",
   "metadata": {},
   "source": [
    "## 12. Load Saved Model (for later use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c51d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(adapter_path: str, base_model_name: str = MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned model from saved adapter.\n",
    "\n",
    "    Args:\n",
    "        adapter_path: Path to saved LoRA adapter\n",
    "        base_model_name: Name of the base model\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "\n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # Load LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Example: Uncomment to load saved model\n",
    "# loaded_model, loaded_tokenizer = load_finetuned_model(adapter_path)\n",
    "# print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650dd32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Loading**: Loading exported data from FixMyCodeDB (JSON/CSV)\n",
    "2. **Data Preprocessing**: Extracting labels and creating conversation-style prompts\n",
    "3. **Train/Test Split**: 80/20 split for training and evaluation\n",
    "4. **Model Loading**: Using 4-bit quantization to fit in 12GB VRAM\n",
    "5. **LoRA Configuration**: Low-rank adaptation for efficient fine-tuning\n",
    "6. **Training**: Using SFTTrainer with memory-efficient settings\n",
    "7. **Visualization**: Plotting training and evaluation loss curves\n",
    "8. **Inference**: Testing the fine-tuned model on new code samples\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Collect more training data using the FixMyCodeDB scraper\n",
    "- Experiment with different base models (Phi-2, CodeLlama)\n",
    "- Adjust LoRA hyperparameters (r, alpha) for better performance\n",
    "- Implement evaluation metrics (precision, recall, F1) for bug detection\n",
    "- Deploy the model for production use"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
